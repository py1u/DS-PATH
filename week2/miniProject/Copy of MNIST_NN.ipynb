{"cells":[{"cell_type":"markdown","id":"P8WPEKzDLSSJ","metadata":{"id":"P8WPEKzDLSSJ"},"source":["# Activity M5: Exploring Neural Networks with MNIST\n","\n","Welcome to Activity M5! In this activity, we will dive into the fascinating world of neural networks, using them to solve a classic problem in machine learning: digit recognition. Our primary tool will be the MNIST dataset, a collection of handwritten digits widely used for training and testing in the field of machine learning.\n","\n","The purpose of this activity is twofold:\n","1. **Understanding Neural Networks**: We'll explore the basic components and functioning of neural networks, including layers, neurons, and activation functions. This will be a hands-on experience to understand how neural networks can learn from data.\n","2. **Practical Application and Experimentation**: You'll get to apply your understanding by experimenting with different hyperparameters of a simple neural network model. The challenge is to improve the model's accuracy on digit recognition while making minimal and thoughtful changes to the hyperparameters.\n","\n","By the end of this activity, you should have a foundational understanding of how neural networks work and how their performance can be influenced by different hyperparameters. This is a crucial step in your journey into the world of artificial intelligence.\n","\n","Let's get started!\n"]},{"cell_type":"markdown","id":"ZImWbNpylNj6","metadata":{"id":"ZImWbNpylNj6"},"source":["## MNIST Dataset Overview\n","\n","### What is the MNIST Dataset?\n","\n","The MNIST (Modified National Institute of Standards and Technology) dataset is a large collection of handwritten digits commonly used for training various image processing systems. It's widely used in the field of machine learning and computer vision for benchmarking classification algorithms.\n","\n","### Dataset Specifications\n","\n","- **Content**: The dataset contains 70,000 images of handwritten digits (0 through 9).\n","- **Image Size**: Each image is 28x28 pixels, represented in a grayscale format.\n","- **Split**: Typically, the dataset is split into 60,000 training images and 10,000 testing images, allowing for robust training and evaluation of models.\n","\n","### Historical Background\n","[Here](https://en.wikipedia.org/wiki/MNIST_database) is the wikipage of MNIST dataset please go through it.\n","\n","\n","We will be using popular deep learning framework tensorflow to complete this activity. First let's load the dataset from tensorflow.keras.datasets and display some instances from the dataset."]},{"cell_type":"code","execution_count":3,"id":"xjCIoSanlLxg","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":509},"executionInfo":{"elapsed":13984,"status":"ok","timestamp":1719519742878,"user":{"displayName":"Mason Audet","userId":"13839816105973126894"},"user_tz":420},"id":"xjCIoSanlLxg","outputId":"7763741a-baba-43cb-cb83-5c0ebf0f4b05"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"]}],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Load MNIST dataset\n","mnist = tf.keras.datasets.mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Function to display a grid of images\n","def display_images(images, labels, num_rows=2, num_cols=5):\n","    plt.figure(figsize=(10, 5))\n","    for i in range(num_rows * num_cols):\n","        plt.subplot(num_rows, num_cols, i + 1)\n","        plt.imshow(images[i], cmap='gray')\n","        plt.title(f\"Label: {labels[i]}\")\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Select random images to display\n","num_images = 10\n","random_indices = np.random.choice(range(len(x_train)), num_images, replace=False)\n","selected_images = x_train[random_indices]\n","selected_labels = y_train[random_indices]\n","\n","# Display the images\n","display_images(selected_images, selected_labels)\n"]},{"cell_type":"markdown","id":"mcPqX7BJOlzO","metadata":{"id":"mcPqX7BJOlzO"},"source":["# Understanding Neural Networks\n","\n","Neural networks are a cornerstone of modern artificial intelligence, particularly in the field of deep learning. They are inspired by the structure and function of the human brain, especially in how neurons process and transmit information.\n","\n","## What is a Neural Network?\n","\n","A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In essence, it is a system of interconnected units (neurons) that work together to process and analyze data.\n","\n","## How Do Neural Networks Work?\n","\n","Neural networks operate on layers of neurons. Each neuron in a layer is connected to neurons in the previous and next layers. These connections have associated weights and biases, which are adjusted during the training process.\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*_SH7tsNDTkGXWtZb.png\" width=\"500\" height=\"300\">\n","\n","\n","\n","- **Input Layer**: This is where the network receives its input data.\n","- **Hidden Layers**: These layers perform computations using activated weights and are the main computational engine of the neural network.\n","- **Output Layer**: The final layer that outputs the prediction or classification.\n","\n","The process of a neural network can be summarized in three key steps:\n","1. **Feedforward**: Input data is passed through the layers of the network. Each neuron applies a weighted sum on the inputs, adds a bias, and then passes it through an activation function.\n","2. **Backpropagation**: The network compares the output it produced with the actual desired output and calculates the error.\n","3. **Weight Adjustment**: Using algorithms like Gradient Descent, the network adjusts its weights and biases to minimize the error.\n","\n","Mathematically, the operation in each neuron can be represented as:\n","\n","$$\n","y = f(\\sum_{i}(w_i \\cdot x_i) + b)\n","$$\n","\n","Where:\n","\n","- $y$ is the output.\\\\\n","- $f$ is the activation function.\n","- $w_i$ are the weights.\n","- $x_i$ are the inputs.\n","- $b$ is the bias.\n","\n","## Learning Resources\n","\n","To gain a deeper understanding of neural networks, I highly recommend watching this excellent video by [3Blue1Brown](https://www.youtube.com/watch?v=aircAruvnKk) on YouTube. It provides a clear and intuitive explanation of how neural networks function.\n","\n","[![Neural Networks Video](https://img.youtube.com/vi/aircAruvnKk/0.jpg)](https://www.youtube.com/watch?v=aircAruvnKk)\n","\n","Additionally, for an interactive learning experience, check out the [3Blue1Brown Neural Network Visualization](https://www.3blue1brown.com/lessons/neural-networks). This interactive blog allows you to draw digits and see how a neural network processes your input in real-time, offering a unique perspective on how neural networks make predictions.\n","\n","\n"]},{"cell_type":"markdown","id":"vl8NsPTXXrAQ","metadata":{"id":"vl8NsPTXXrAQ"},"source":["# Understanding Neural Network Hyperparameters\n","\n","In neural networks, hyperparameters are the parameters whose values are set before the learning process begins. These parameters have a significant impact on the training of the network and the final results. Let's discuss some of the essential hyperparameters:\n","\n","## Number of Layers\n","\n","- **Input Layer**: The first layer that receives the input data. Its size is determined by the dimensions of the input data.\n","- **Hidden Layers**: Layers between the input and output layers. The number of hidden layers and their size (number of neurons) can greatly affect the network's ability to capture complex patterns.\n","- **Output Layer**: The final layer that produces the output. Its size is determined by the number of output classes or values.\n","\n","## Number of Neurons in a Layer\n","\n","- The number of neurons in a layer represents the layer's capacity to learn various aspects of the data. More neurons can increase the network's complexity and computational cost.\n","\n","## Activation Functions\n","\n","Activation functions introduce non-linear properties to the network, allowing it to learn more complex data patterns.\n","\n","- **ReLU (Rectified Linear Unit)**: Commonly used in hidden layers, ReLU is defined as $f(x) = max(0, x)$. It helps with faster training and mitigating the vanishing gradient problem. Also calculating the gradient of ReLU function is simpler. It is 1 for values of x greater than 0 and 0 otherwise.\n","\n","  <img src=\"https://miro.medium.com/v2/resize:fit:1384/1*_vvB81JFM1PGZvYeVI52XQ.png\" width=\"500\" height=\"300\">\n","\n","- **Sigmoid**: Often used in the output layer for binary classification, it squashes the output between 0 and 1, defined as $f(x) = \\frac{1}{1 + e^{-x}}$. It's useful for models where we need to predict the probability as an output.\n","\n","  <img src=\"https://miro.medium.com/v2/resize:fit:970/1*Xu7B5y9gp0iL5ooBj7LtWw.png\" width=\"500\" height=\"300\">\n","\n","- **Softmax**: The Softmax function is often used in the output layer of a neural network for multi-class classification problems. It is simply the extension of sigmoid function into multiclass problems. It converts the output scores from the network into probability values for each class. The function is defined as:\n","\n","$$\n","\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n","$$\n","\n","where $x_i$ is the score (also known as the logit) for class i  and the denominator is the sum of exponential scores for all classes. This function ensures that the output probabilities sum up to 1, making it a suitable choice for probabilistic interpretation in classification tasks.\n","\n","\n","## Learning Rate\n","\n","- The learning rate defines how quickly or slowly a neural network updates its parameters during training. A too high learning rate can cause the model to converge too quickly to a suboptimal solution, while a too low learning rate can make the training process unnecessarily long.\n","\n","Understanding and tuning these hyperparameters is crucial for training effective neural networks. Different types of problems may require different configurations for optimal performance.\n","\n"]},{"cell_type":"markdown","id":"PNwCISkirC5y","metadata":{"id":"PNwCISkirC5y"},"source":["## Neural Network Training on the MNIST Dataset\n","\n","### Overview\n","In this exercise, we start with a neural network model that is initially configured to perform suboptimally on the MNIST digit classification task. This setup serves as a practical exercise in understanding and optimizing neural network hyperparameters for image recognition and machine learning.\n","\n","### Data Preparation\n","The MNIST dataset, consisting of grayscale images of handwritten digits (0-9), is loaded and split into training and test sets. Each image, originally in a 28x28 pixel format, is normalized to ensure pixel values are in the range [0, 1]. This normalization is crucial for consistent input value scales, aiding in the training process.\n","\n","### Initial Neural Network Architecture\n","- The initial model is a sequential feedforward neural network with two hidden layers.\n","- **Input Layer**: A flattening layer that transforms each 28x28 image into a 1D array of 784 features.\n","- **Hidden Layers**: Two dense layers, but with configurations that are not optimal for learning the complex patterns in the data effectively.\n","- **Output Layer**: A final dense layer designed for multi-class classification but may not be optimized for best performance.\n","\n","### Training Process\n","- The model is compiled with an optimizer and loss function, but the initial settings might not be ideal for this specific task.\n","- Training is conducted over several epochs, and the model's performance on the test set is evaluated at the end of each epoch. However, the initial training might not yield high accuracy due to suboptimal hyperparameter settings.\n","\n"]},{"cell_type":"markdown","id":"AxdgRercggoi","metadata":{"id":"AxdgRercggoi"},"source":["# A crash course on tensorflow\n","\n","In TensorFlow, defining and compiling a neural network involves several key steps. Each step allows you to specify certain hyperparameters that control the network's architecture and learning process.\n","\n","## Defining the Neural Network\n","\n","### 1. Model Architecture\n","- **Sequential Model**: In TensorFlow, a common way to build a neural network is by using the `Sequential` model from `tensorflow.keras`. This model allows layers to be added in sequence.\n","  ```python\n","  from tensorflow.keras.models import Sequential\n","  model = Sequential()\n","  ```\n","- **Layers**: The layers are added to the model using the `.add()` method. Each layer can have its own hyperparameters.\n","  - **Dense Layer**: A fully connected layer where each neuron receives input from all neurons of the previous layer.\n","    ```python\n","    from tensorflow.keras.layers import Dense\n","    model.add(Dense(units=64, activation='relu'))\n","    ```\n","    - `units`: Number of neurons in the layer.\n","    - `activation`: The activation function for the layer.\n","\n","### 2. Input Layer\n","- The first layer of the network needs to know the input shape, so the `input_shape` argument is often specified in the first layer.\n","  ```python\n","  model.add(Dense(64, activation='relu', input_shape=(input_dimension,)))\n","  ```\n","\n","## Compiling the Neural Network\n","\n","After defining the model architecture, the next step is to compile the model. This step involves specifying the optimizer, loss function, and metrics for evaluation.\n","\n","### 1. Optimizer\n","- The optimizer is an algorithm or method used to change the attributes of the neural network such as weights and learning rate. It helps in minimizing the loss function.\n","  ```python\n","  from tensorflow.keras.optimizers import Adam\n","  model.compile(optimizer=Adam(learning_rate=0.001))\n","  ```\n","\n","### 2. Loss Function\n","- The loss function measures how well the model is performing. A common choice for classification tasks is the categorical cross-entropy.\n","  ```python\n","  model.compile(loss='sparse_categorical_crossentropy')\n","  ```\n","\n","### 3. Metrics\n","- Metrics are used to evaluate the performance of your model. Accuracy is a common metric.\n","  ```python\n","  model.compile(metrics=['accuracy'])\n","  ```\n","\n","### Full Compilation Example\n","- Here's how the model is typically compiled with all three components:\n","  ```python\n","  model.compile(optimizer='adam',\n","                loss='sparse_categorical_crossentropy',\n","                metrics=['accuracy'])\n","  ```\n","\n","This setup forms the basis of most neural network models in TensorFlow, and understanding these components is crucial for effective model training and evaluation.\n"]},{"cell_type":"markdown","id":"oPnEkq9kdPmt","metadata":{"id":"oPnEkq9kdPmt"},"source":["# Activity Objective\n","\n","In this activity, you are presented with a neural network that has been intentionally configured to perform poorly on the MNIST digit classification task. Your objective is to modify the hyperparameters of this network to significantly improve its accuracy.\n","\n","## Your Task\n","\n","1. **Analyze the Current Model**: Understand the current structure and configuration of the neural network. Identify potential reasons for its low performance.\n","\n","2. **Modify Hyperparameters**: Adjust the hyperparameters of the model. This includes but is not limited to:\n","   - The number of neurons in each layer.\n","   - The activation functions used in each layer.\n","   - The learning rate for the training process.\n","   - The number of layers in the network.\n","\n","3. **Achieve Higher Accuracy**: Your goal is to achieve an accuracy of over 95% on the MNIST test dataset. However, your changes should be thoughtful and minimal. Avoid extreme solutions like excessively large networks that do not align with the complexity of the task.\n","\n","4. **Reflect on the Results**: After improving the model, reflect on:\n","   - Why the initial model performed poorly.\n","   - How your changes improved the model's performance.\n","   - Any additional insights you gained about neural network optimization.\n","5. **Upload the ipynb File to Gradescope**: Once you have completed the activity and made your changes and run the model to achieve accuracy over 95%, upload your Jupyter Notebook (ipynb file) to Gradescope for grading. Ensure that your notebook is well-organized.Make sure to save and upload the final version of your notebook that includes the trained model and your analysis.\n","\n","\n","## Guidelines\n","\n","- **Balance Complexity and Performance**: Aim for a model that is as simple as possible while still achieving high accuracy.\n","- **Experimentation**: Feel free to experiment with different combinations of hyperparameters. However, ensure that each change is justified and contributes to your understanding of neural network behavior.\n","- **Understanding Over Memorization**: Focus on understanding the impact of each hyperparameter on the model's learning and generalization capabilities rather than trial-and-error.\n","\n","Good luck, and enjoy the process of tuning and learning from your neural network model!\n"]},{"cell_type":"code","execution_count":null,"id":"04Kj7421om_9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115894,"status":"ok","timestamp":1701905739704,"user":{"displayName":"Taneesha Sharma","userId":"02603635008537462274"},"user_tz":480},"id":"04Kj7421om_9","outputId":"9e99c915-2dfa-4df9-db6c-5590be0cf114"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","1868/1875 [============================>.] - ETA: 0s - loss: 0.2197 - accuracy: 0.9349Epoch 1: Training Accuracy: 0.9350, Test Accuracy: 0.9666\n","1875/1875 [==============================] - 9s 4ms/step - loss: 0.2193 - accuracy: 0.9350 - val_loss: 0.1078 - val_accuracy: 0.9666\n","Epoch 2/15\n","1874/1875 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9735Epoch 2: Training Accuracy: 0.9735, Test Accuracy: 0.9748\n","1875/1875 [==============================] - 8s 4ms/step - loss: 0.0867 - accuracy: 0.9735 - val_loss: 0.0867 - val_accuracy: 0.9748\n","Epoch 3/15\n","1863/1875 [============================>.] - ETA: 0s - loss: 0.0554 - accuracy: 0.9831Epoch 3: Training Accuracy: 0.9830, Test Accuracy: 0.9751\n","1875/1875 [==============================] - 7s 4ms/step - loss: 0.0555 - accuracy: 0.9830 - val_loss: 0.0889 - val_accuracy: 0.9751\n","Epoch 4/15\n","1875/1875 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9864Epoch 4: Training Accuracy: 0.9864, Test Accuracy: 0.9772\n","1875/1875 [==============================] - 8s 4ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.0834 - val_accuracy: 0.9772\n","Epoch 5/15\n","1868/1875 [============================>.] - ETA: 0s - loss: 0.0299 - accuracy: 0.9906Epoch 5: Training Accuracy: 0.9906, Test Accuracy: 0.9742\n","1875/1875 [==============================] - 7s 4ms/step - loss: 0.0300 - accuracy: 0.9906 - val_loss: 0.0978 - val_accuracy: 0.9742\n","Epoch 6/15\n","1870/1875 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9923Epoch 6: Training Accuracy: 0.9923, Test Accuracy: 0.9777\n","1875/1875 [==============================] - 8s 4ms/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.0823 - val_accuracy: 0.9777\n","Epoch 7/15\n","1868/1875 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9930Epoch 7: Training Accuracy: 0.9930, Test Accuracy: 0.9799\n","1875/1875 [==============================] - 8s 4ms/step - loss: 0.0199 - accuracy: 0.9930 - val_loss: 0.0839 - val_accuracy: 0.9799\n","Epoch 8/15\n","1875/1875 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9942Epoch 8: Training Accuracy: 0.9942, Test Accuracy: 0.9800\n","1875/1875 [==============================] - 7s 4ms/step - loss: 0.0169 - accuracy: 0.9942 - val_loss: 0.0896 - val_accuracy: 0.9800\n","Epoch 9/15\n","1873/1875 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9940Epoch 9: Training Accuracy: 0.9940, Test Accuracy: 0.9770\n","1875/1875 [==============================] - 8s 4ms/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 0.1104 - val_accuracy: 0.9770\n","Epoch 10/15\n","1871/1875 [============================>.] - ETA: 0s - loss: 0.0145 - accuracy: 0.9952Epoch 10: Training Accuracy: 0.9952, Test Accuracy: 0.9789\n","1875/1875 [==============================] - 7s 4ms/step - loss: 0.0145 - accuracy: 0.9952 - val_loss: 0.1073 - val_accuracy: 0.9789\n","Epoch 11/15\n","1862/1875 [============================>.] - ETA: 0s - loss: 0.0107 - accuracy: 0.9963Epoch 11: Training Accuracy: 0.9963, Test Accuracy: 0.9791\n","1875/1875 [==============================] - 8s 4ms/step - loss: 0.0107 - accuracy: 0.9963 - val_loss: 0.1046 - val_accuracy: 0.9791\n","Epoch 12/15\n","1867/1875 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9954Epoch 12: Training Accuracy: 0.9955, Test Accuracy: 0.9809\n","1875/1875 [==============================] - 7s 4ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.1015 - val_accuracy: 0.9809\n","Epoch 13/15\n","1863/1875 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9971Epoch 13: Training Accuracy: 0.9971, Test Accuracy: 0.9753\n","1875/1875 [==============================] - 8s 4ms/step - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.1272 - val_accuracy: 0.9753\n","Epoch 14/15\n","1866/1875 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9964Epoch 14: Training Accuracy: 0.9964, Test Accuracy: 0.9819\n","1875/1875 [==============================] - 8s 4ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 0.0988 - val_accuracy: 0.9819\n","Epoch 15/15\n","1864/1875 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9968Epoch 15: Training Accuracy: 0.9968, Test Accuracy: 0.9798\n","1875/1875 [==============================] - 7s 4ms/step - loss: 0.0103 - accuracy: 0.9968 - val_loss: 0.1166 - val_accuracy: 0.9798\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x797f3769e200>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.optimizers import Adam\n","\n","# Load MNIST dataset\n","mnist = tf.keras.datasets.mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Normalize the pixel values (0-255) to the 0-1 range\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","# Define the model\n","model = Sequential([\n","    Flatten(input_shape=(28, 28)),             # Flatten the 28x28 images to 1D array of 784 features\n","    Dense(1000, activation='relu'),            #used relu to test if capuring non-linearities helps increasing accuracy and reducing loss\n","    Dense(10, activation='relu'),\n","    Dense(10, activation='softmax')            # Output layer with 10 neurons (one per class) and softmax activation\n","])\n","\n","# Compile the model with a learning rate and loss function\n","model.compile(optimizer=Adam(learning_rate=0.001),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Function to print training and test accuracy after every epoch\n","class AccuracyHistory(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        train_acc = logs['accuracy']\n","        test_acc = self.model.evaluate(x_test, y_test, verbose=0)[1]\n","        print(f'Epoch {epoch+1}: Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n","\n","# Create an instance of the accuracy history class\n","accuracy_history = AccuracyHistory()\n","\n","# Train the model\n","model.fit(x_train, y_train,\n","          epochs=15,\n","          validation_data=(x_test, y_test),\n","          callbacks=[accuracy_history])\n"]},{"cell_type":"code","execution_count":null,"id":"Twp8sWquahI3","metadata":{"id":"Twp8sWquahI3"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":5}
